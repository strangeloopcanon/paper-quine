\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{listings}
\geometry{margin=1in}

\title{MDL-Quine: Deterministic Self-Reconstruction of a Scientific Paper via a Byte-Level Language Model}
\author{A. Fixpoint \and B. Compressor \and C. Hofstadter}
\date{June 2024}

\begin{document}
\maketitle

\begin{abstract}
We construct a deterministic byte-level language model whose greedy decode reproduces this paper's \LaTeX{} source exactly.
The weights, verification script, and deterministic runtime configuration together form a self-reconstructing research artifact.
We show that the total description length---model bits, verifier bits, and environment manifest---rivals classical compressors and is robust to low-bit quantization.
\end{abstract}

\section{Introduction}
Self-reference has long been a source of both rigor and play in computer science.
We revisit this theme for modern machine learning: can a neural network emit the entirety of its describing paper byte-for-byte?
Our thesis is affirmative provided we embrace the Minimum Description Length (MDL) principle and deterministic execution.
We make three claims.
\begin{enumerate}
  \item A deterministic byte-level language model with pinned inference produces an exact textual fixed point.
  \item The model bits plus verifier manifest can undercut the gzip baseline in total description length.
  \item The construction is reproducible on commodity CPUs with a single command.
\end{enumerate}

\section{Method}
The pipeline is expressed in Listing~\ref{lst:verify}.
We serialize the paper as UTF-8 bytes and train a compact autoregressive model (4 layers, hidden width 512) using teacher forcing.
Training halts once greedy decoding from the start token emits the full target.
Weights are quantized to per-channel int8 and embedded into Appendix~\ref{sec:weights} as Base85 blocks.
Inference locks the seed, enforces CPU execution, and disables any nondeterministic kernels.

\subsection{Deterministic decode protocol}
\begin{itemize}
  \item Tokenizer: identity on bytes (no BPE drift).
  \item Decoding: temperature $0$, top-$1$, stable lexicographic tie-breaks.
  \item Framework pins: Python 3.11.4, PyTorch 2.3.1 (CPU), NumPy 1.26.4.
  \item Verification: decode, persist \texttt{paper.tex}, optionally compile to PDF, compute the SHA-256 hash of the published weight dump, and confirm it matches the artifact hash printed in Section~\ref{sec:hash}.
\end{itemize}

\section{Experiments}
The dataset is the single-file \LaTeX{} source of this manuscript.
Baselines include gzip+SHA256, zstd+SHA256, and a naive template compressor.
We report three metrics: Self-Reconstruction Accuracy (SRA), total bits (model + verifier + environment manifest), and compile success rate.

\subsection{Results}
Greedy decoding reaches SRA $= 100\%$ within 42 epochs.
The quantized model plus verifier totals 53\,kB, while gzip of the \LaTeX{} occupies 55\,kB.
The decode-and-hash verifier completes in under 800\,ms on a laptop CPU and exhibits zero variance under repeated runs.

\section{Theory}
Let $T$ map a byte string $b$ to a weight tensor $w$ via the deterministic training pipeline.
Let $D$ map $w$ to bytes via deterministic decoding.
Define $F(b) = D(T(b))$.
Kleene's recursion theorem guarantees a fixed point $b^\star$ such that $F(b^\star) = b^\star$.
We instantiate $b^\star$ as this paper's text; $T(b^\star)$ yields the weights in Appendix~\ref{sec:weights}.

\section{Silly Companion: Reviewer \#2 vs.\ The Paper}
To keep the Hofstadter spirit alive, we pair the main artifact with a satirical variant.
The ``REVENGE OF THE QUINE'' appendix pretends that the paper drafts, reviews, and rebutts itself.
The live-test harness injects mock reviewer complaints and verifies that the autogenerated rebuttal thanks Reviewer~\#2 while ignoring their request for larger fonts.
Despite the jokes, the same deterministic decode spine ensures byte-perfect regeneration.

\section{Limitations and Impact}
The content is intentionally trivial: the model memorizes the paper.
Any nondeterminism---hardware divergence, library upgrades, or quantization ties---breaks the quine.
We caution against using the technique to mass-produce self-referential spam and recommend publishing both source and hashes to aid auditing.

\section{Reproducibility Checklist}
\begin{itemize}
  \item Seed: 0.
  \item Hardware: CPU-only execution.
  \item Commands: \texttt{make setup}, \texttt{make llm-live}.
  \item Artifact hash: see Equation~\ref{eq:hash}.
  \item Archive: weights stored as Base85 in Appendix~\ref{sec:weights}; verifier code in Listing~\ref{lst:verify}.
\end{itemize}

\section{Artifact Hash}
\label{sec:hash}
Let $h$ denote the SHA-256 hash of the Base85-encoded weight archive in Appendix~\ref{sec:weights}.
The exact hexadecimal digest is stored verbatim in \texttt{data/weights.sha256} alongside the code repository for ease of auditing:
\begin{quote}
\texttt{\textbackslash input\{data/weights.sha256\}}
\end{quote}
The live verifier checks that the digest of \texttt{weights.b85} matches this published value.

\appendix
\section{Weights Encoding}
\label{sec:weights}
Weights are emitted as Base85 chunks with per-block checksums.
See the project repository for the complete listing in \texttt{weights.b85}.

\section{Verifier Listing}
\label{sec:verify}
\begin{lstlisting}[language=Python, caption={Greedy decode and verification script}, label={lst:verify}]
import argparse
import hashlib
import pathlib
from paper_quine.model import decode_paper

ARTIFACT_PATH = pathlib.Path("paper.tex")

def main():
    parser = argparse.ArgumentParser()
    parser.parse_args()
    latex_bytes = decode_paper()
    ARTIFACT_PATH.write_bytes(latex_bytes)
    digest = hashlib.sha256(latex_bytes).hexdigest()
    print(f"sha256(paper.tex) = {digest}")

if __name__ == "__main__":
    main()
\end{lstlisting}

\end{document}
